{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os, os.path\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import sys\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import echonet\n",
    "import wget \n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.video.r2plus1d_18(pretrained=True)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./weights/r2plus1d_18_32_2_pretrained.pt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DestinationForWeights = \"./weights\"\n",
    "StanfordEFWeightsURL = 'https://github.com/douyang/EchoNetDynamic/releases/download/v1.0.0/r2plus1d_18_32_2_pretrained.pt'\n",
    "os.path.join(DestinationForWeights, os.path.basename(StanfordEFWeightsURL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading EF Weights,  https://github.com/douyang/EchoNetDynamic/releases/download/v1.0.0/r2plus1d_18_32_2_pretrained.pt  to  ./weights/r2plus1d_18_32_2_pretrained.pt\n"
     ]
    }
   ],
   "source": [
    "# スタンフォードの重みを取得\n",
    "DestinationForWeights = \"./weights\"\n",
    "StanfordEFWeightsURL = 'https://github.com/douyang/EchoNetDynamic/releases/download/v1.0.0/r2plus1d_18_32_2_pretrained.pt'\n",
    "\n",
    "if not os.path.exists(os.path.join(DestinationForWeights, os.path.basename(StanfordEFWeightsURL))):\n",
    "    print(\"Downloading EF Weights, \", StanfordEFWeightsURL,\" to \",os.path.join(DestinationForWeights,os.path.basename(StanfordEFWeightsURL)))\n",
    "    filename = wget.download(StanfordEFWeightsURL, out = DestinationForWeights)\n",
    "else:\n",
    "    print(\"EF Weights already present\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"loading weights from \", os.path.join(DestinationForWeights, \"r2plus1d_18_32_2_pretrained.pt\"))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda is available, original weights\")\n",
    "#     device = torch.device(\"cuda\")\n",
    "    device = torch.device(f'cuda:{0}')\n",
    "    model = torch.nn.DataParallel(model, device_ids=[0, 1, 2,3 ]).cuda()\n",
    "    model.to(device)\n",
    "    checkpoint = torch.load(os.path.join(DestinationForWeights, \"r2plus1d_18_32_2_pretrained.pt\"))\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "else:\n",
    "    print(\"cuda is not available, cpu weights\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    checkpoint = torch.load(os.path.join(DestinationForWeights, os.path.basename(ejectionFractionWeightsURL)), map_location = \"cpu\")\n",
    "    state_dict_cpu = {k[7:]: v for (k, v) in checkpoint['state_dict'].items()}\n",
    "    model.load_state_dict(state_dict_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中間層重みの凍結\n",
    "\n",
    "# for i, (name, param) in enumerate(model.named_parameters()):\n",
    "#     if i <= 83:\n",
    "#         param.requires_grad = False\n",
    "#     print(i, name)\n",
    "#     print(param.shape)\n",
    "#     print(param.requires_grad)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-6, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00,  9.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1197\n",
      "462\n",
      "455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "length = 32\n",
    "period = 1\n",
    "n_clips = 2\n",
    "\n",
    "mean, std = echonet.utils.get_mean_and_std(echonet.datasets.Echo(split='train'))\n",
    "\n",
    "train_dataset = echonet.datasets.Echo(split=\"train\", clips = n_clips, mean=mean, std=std, length=length, period=period)\n",
    "val_dataset= echonet.datasets.Echo(split=\"valid\", clips = n_clips, mean=mean, std=std, length=length, period=period)\n",
    "test_dataset = echonet.datasets.Echo(split=\"test\", clips = n_clips, mean=mean, std=std, length=length, period=period)\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch_train = 8\n",
    "n_batch_val = 8\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=n_batch_train, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=n_batch_val, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3, verbose=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# reduce処理ありのauc\n",
    "for epoch in range(200):\n",
    "    losses_train = []\n",
    "    t_true_train = []\n",
    "    t_pred_train = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i, (x, y) in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "        x_len = x.shape[1]\n",
    "        t_true_train.extend(y.tolist())\n",
    "        n_batch = x.shape[0]\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        x = x.reshape(-1, 3, length, 112, 112).to(device)\n",
    "        y = y.reshape(-1, 1).type(torch.DoubleTensor).to(device)\n",
    "        # forward + backward + optimize\n",
    "    \n",
    "        outputs = model(x)\n",
    "        \n",
    "        outputs_reduce = torch.from_numpy(np.zeros((n_batch, 1))).clone().to(device)\n",
    "        for i in range(n_batch):\n",
    "            mean = outputs[i*n_clips: i*n_clips+3].mean()\n",
    "            outputs_reduce[i] = mean\n",
    "\n",
    "        loss = criterion(outputs_reduce, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        losses_train.append(loss.tolist())\n",
    "        \n",
    "        outputs_sigmoid = torch.sigmoid(outputs_reduce)\n",
    "        t_pred_train.extend(outputs_sigmoid.reshape(-1).tolist())\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    losses_val = []\n",
    "    t_true_val = []\n",
    "    t_pred_val= []\n",
    "    \n",
    "    for i, (x, y) in enumerate(tqdm(val_dataloader)):\n",
    "        x_len = x.shape[1]\n",
    "        t_true_val.extend(y.tolist())\n",
    "        n_batch = x.shape[0]\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        x = x.reshape(-1, 3, length, 112, 112).to(device)\n",
    "        y = y.reshape(-1, 1).type(torch.DoubleTensor).to(device)\n",
    "        # forward + backward + optimize\n",
    "    \n",
    "        outputs = model(x)\n",
    "        \n",
    "        outputs_reduce = torch.from_numpy(np.zeros((n_batch, 1))).clone().to(device)\n",
    "        for i in range(n_batch):\n",
    "            mean = outputs[i*n_clips: i*n_clips+3].mean()\n",
    "            outputs_reduce[i] = mean\n",
    "            \n",
    "        loss = criterion(outputs_reduce, y)\n",
    "        # print statistics\n",
    "        losses_val.append(loss.tolist())\n",
    "        \n",
    "        outputs_sigmoid = torch.sigmoid(outputs_reduce)\n",
    "        t_pred_val.extend(outputs_sigmoid.reshape(-1).tolist())\n",
    "    \n",
    "    train_loss.append(np.mean(losses_train))\n",
    "    val_loss.append(np.mean(losses_val))\n",
    "    scheduler.step(np.mean(losses_val))\n",
    "    \n",
    "    print('EPOCH: {}, Train[{:.3f}, AUC: {:.3f}], Val[{:.3f}, AUC: {:.3f}]'.format(\n",
    "        epoch,\n",
    "        np.mean(losses_train),\n",
    "        roc_auc_score(t_true_train, t_pred_train),\n",
    "        np.mean(losses_val),\n",
    "        roc_auc_score(t_true_val, t_pred_val)\n",
    "    ))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test - with reducing\n",
    "pred_test_all = []\n",
    "for i in range(1):\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    model.eval()\n",
    "    losses_test = []\n",
    "    t_true_test = []\n",
    "    t_pred_test= []\n",
    "\n",
    "    for i, (x, y) in enumerate(tqdm(test_dataloader)):\n",
    "            x_len = x.shape[1]\n",
    "            t_true_test.extend(y.tolist())\n",
    "            n_batch = x.shape[0]\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            x = x.reshape(-1, 3, length, 112, 112).to(device)\n",
    "            y = y.reshape(-1, 1).type(torch.DoubleTensor).to(device)\n",
    "            # forward + backward + optimize\n",
    "\n",
    "            outputs = model(x)\n",
    "            outputs_reduce = torch.from_numpy(np.zeros((n_batch, 1))).clone().to(device)\n",
    "            for i in range(n_batch):\n",
    "                mean = outputs[i*n_clips: i*n_clips+3].mean()\n",
    "                outputs_reduce[i] = mean\n",
    "\n",
    "            loss = criterion(outputs_reduce, y)\n",
    "            # print statistics\n",
    "            losses_test.append(loss.tolist())\n",
    "\n",
    "            outputs_sigmoid = torch.sigmoid(outputs_reduce)\n",
    "            t_pred_test.extend(outputs_sigmoid.reshape(-1).tolist())\n",
    "\n",
    "    print('Test{:.3f}, AUC: {:.3f}'.format(\n",
    "            np.mean(losses_test),\n",
    "            roc_auc_score(t_true_test, t_pred_test))\n",
    "        )\n",
    "    pred_test_all.append(t_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_all = np.array(pred_test_all).T\n",
    "pred_test_mean = pred_test_all.mean(axis = 1)\n",
    "roc_auc_score(t_true_test, pred_test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(train_loss, linewidth=3, label=\"train\")\n",
    "plt.plot(val_loss, linewidth=3, label=\"validation\")\n",
    "plt.title(\"Learning curve\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(t_true_test, pred_test_mean)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title(\"TEST AUC : {}\".format(str(roc_auc_score(t_true_test, pred_test_mean))))\n",
    "plt.xlabel('FPR: False positive rate')\n",
    "plt.ylabel('TPR: True positive rate')\n",
    "plt.grid()\n",
    "# plt.savefig('data/dst/sklearn_roc_curve.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./weights/model1108_sgd_stanford_lr1e5_16_2_3_nb16.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
